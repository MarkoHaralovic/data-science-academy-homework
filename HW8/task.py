# -*- coding: utf-8 -*-
"""task.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p1GAEHzBJUTm8vxKudXTVWEGm0wiCiba
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import logging
logging.basicConfig(level=logging.INFO)

"""Dataset was gathered using the code from the second task,and saved in this directory."""

dataset = pd.read_pickle('dataset_academy.pickle')

logging.info(f"Dataset length :{len(dataset)}")
logging.info(f"Dataset columns :{dataset.columns}")

dataset.reset_index(drop=True, inplace=True)
dataset.head()

dataset['campaign'].value_counts()

for columns in dataset.columns:
   logging.info(f"Number if missing values in {columns} : {dataset[columns].isnull().sum()}")

logging.info(f"Different countries in the dataset : {dataset['geo_country'].nunique()}")

logging.info(f"Different campaigns represented in the dataset : {dataset['campaign'].nunique()}")

# define classes: class 1 are organic users and class 2 are campaign users
dataset['user_class'] = np.where(dataset['campaign'] == '<organic>', 0, 1)

for column in dataset.columns:
   if dataset[column].dtype == 'float64':
      if np.mean(dataset[column], axis=0) <= 0.1:
       logging.info(f"Column {column} ")

"""Initial thoughts on the model:

- try linear and non linear supervised  model, compare them; then, choose one model for parameter tuning and tune it, then use that model for feature selection desribed below.
- use supervised  model on data, all organic data with high probability of being non organic will be added to the training set and if evaluation on independant samples (validation dataset) is better, keep them changed
- use probabilistic method; starting with certainty of 1 for non organic users to be non organic and soft label of 0.5/0.7 for organic users to be organic, build classifier and update probabilities of organic users, if they drop belove certain tresold, they are added to the non organic users
- use ensamble methods, train multiple models on non organic data and use them to classify organic data, if they mostly agree on the classification, add the user to the non organic users

After obtaining new labels, I will use supervised model for classification.

Comments
- I will probably not add synthetic data, as I lack domain knowledge to be certain in my data ingestion.
"""

dataset.drop(columns=['user_id', 'campaign'], inplace=True)

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
dataset['geo_country'] = label_encoder.fit_transform(dataset['geo_country'])

dataset.head()

organic_users,non_organic_users = dataset[dataset['user_class'] == '1'],dataset[dataset['user_class'] == '2']
organic_users.reset_index(drop=True, inplace=True)
non_organic_users.reset_index(drop=True, inplace=True)

logging.info(f"Number of organic users : {len(organic_users)}")
logging.info(f"Number of non-organic users : {len(non_organic_users)}")

"""Modeling"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve

dataset = dataset.sample(frac=1).reset_index(drop=True)

X = dataset.drop(columns=['user_class'])
y = dataset['user_class']

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)

"""Random Forests"""

from sklearn.ensemble import RandomForestClassifier

randomForest = RandomForestClassifier(n_estimators=100,
                                      criterion='gini',
                                      max_depth=None,
                                      min_impurity_decrease=0.0,
                                      random_state=42
                                      )

randomForest.fit(X_train,y_train)

predictions = randomForest.predict_proba(X_test)

threshold = 0.5

y_pred = np.where(predictions[:,1]<threshold,0,1)

randomForestAccuracy = accuracy_score(y_true = y_test.astype(int) ,
                                      y_pred = y_pred
                                      )
randomForestPrecision = precision_score(y_true = y_test.astype(int) ,
                                        y_pred = y_pred
                                      )
randomForestRecall = recall_score(y_true = y_test.astype(int) ,
                                  y_pred = y_pred
                                  )
randomForestRoc_auc_score = roc_auc_score(y_true = y_test.astype(int),
                                          y_score = y_pred
                                  )

print(f"Accuracy : {randomForestAccuracy}, precision : {randomForestPrecision}, Recall : {randomForestRecall}, ROC AUC Score {randomForestRoc_auc_score}")

cm = confusion_matrix(y_test.astype(int), y_pred)
ConfusionMatrixDisplay(cm).plot()

fpr, tpr, thresholds = roc_curve(y_true=y_test.astype(int),y_score=y_pred)
plt.figure()
plt.plot(fpr, tpr, label='ROC Curve')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

from xgboost import XGBClassifier

"""Max depth
○ Minimum child weight
○ Subsample
○ Learning rate (eta)
○ Number of boosting rounds
"""

xgboost_model = XGBClassifier(
    n_estimators=None,
    max_depth=None,
    grow_policy= None, #'depthwise',
    learning_rate=None
)

xgboost_model.fit(X_train,y_train)

predictions = xgboost_model.predict_proba(X_test)

threshold = 0.5
y_pred = np.where(predictions[:,0]>threshold,0,1)

xgboostAccuracy = accuracy_score(y_true = y_test.astype(int) ,
                                      y_pred = y_pred
                                      )
xgboostPrecision = precision_score(y_true = y_test.astype(int) ,
                                        y_pred = y_pred
                                      )
xgboostRecall = recall_score(y_true = y_test.astype(int) ,
                                  y_pred = y_pred
                                  )
xgboostRoc_auc_score = roc_auc_score(y_true = y_test.astype(int),
                                          y_score = y_pred
                                  )

print(f"Accuracy : {xgboostAccuracy}, precision : {xgboostPrecision}, Recall : {xgboostRecall}, ROC AUC Score {xgboostRoc_auc_score}")

cm = confusion_matrix(y_test, y_pred)
ConfusionMatrixDisplay(cm).plot()

fpr, tpr, thresholds = roc_curve(y_true=y_test,y_score=y_pred)
plt.figure()
plt.plot(fpr, tpr, label='ROC Curve')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

"""Linear model -> Linear SVM"""

from sklearn.svm import LinearSVC as SVM

SVM_model = SVM(penalty='l2',
                loss='hinge',
                class_weight=None,
                random_state=42
                )

SVM_model.fit(X_train,y_train)

predictions = SVM_model.predict(X_test)
y_pred = predictions

SVMAccuracy = accuracy_score(y_true = y_test,
                                      y_pred = y_pred
                                      )
SVMPrecision = precision_score(y_true = y_test,
                                        y_pred = y_pred
                                      )
SVMRecall = recall_score(y_true = y_test,
                                  y_pred = y_pred
                                  )
SVMRoc_auc_score = roc_auc_score(y_true = y_test,
                                          y_score = y_pred
                                  )

print(f"Accuracy : {SVMAccuracy}, precision : {SVMPrecision}, Recall : {SVMRecall}, ROC AUC Score {SVMRoc_auc_score}")

cm = confusion_matrix(y_test, y_pred)
ConfusionMatrixDisplay(cm).plot()

fpr, tpr, thresholds = roc_curve(y_true=y_test,y_score=y_pred)
plt.figure()
plt.plot(fpr, tpr, label='ROC Curve')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

"""Logistic Regression"""

from sklearn.linear_model import LogisticRegression as LR

LR_model = LR(max_iter=1000,
              class_weight=None,
              random_state=42)

LR_model.fit(X_train,y_train)

predictions = LR_model.predict_proba(X_test)

threshold=0.5

y_pred = np.where(predictions[:,0]>threshold,0,1)

LRAccuracy = accuracy_score(y_true = y_test,
                                      y_pred = y_pred
                                      )
LRPrecision = precision_score(y_true = y_test,
                                        y_pred = y_pred
                                      )
LRRecall = recall_score(y_true = y_test,
                                  y_pred = y_pred
                                  )
LRRoc_auc_score = roc_auc_score(y_true = y_test,
                                          y_score = y_pred
                                  )

print(f"Accuracy : {LRAccuracy}, precision : {LRPrecision}, Recall : {LRRecall}, ROC AUC Score {LRRoc_auc_score}")

cm = confusion_matrix(y_test.astype(int), y_pred)
ConfusionMatrixDisplay(cm).plot()

fpr, tpr, thresholds = roc_curve(y_true=y_test,y_score=y_pred)
plt.figure()
plt.plot(fpr, tpr, label='ROC Curve')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

"""Conslusions?
- proceeding with XGBoost, I will try to tune parameters as much as possible, while trying to up the precision metrics
- next, I will try to drop some features or scale them and see the results
- finally, knowing the underlying uncertaily in the data labeling, I will try to update the labels by using the best predicting model up to that point and I will update the classes (for the <organic> data entries) : for predictions the model is highly confident in, the labels will be changed. Starting class probabilities will be obtained by predicting with the best model on the data.

Grid search for optimal XGBoost parameters
"""

from sklearn.model_selection import GridSearchCV

xgboost_model = XGBClassifier(seed=42)

param_grid = {
    'n_estimators': [100, 500, 1000],
    'max_depth': range(3,10,2),
    'subsample': [0.5,0.7,0.9],
    'learning_rate': [0.01, 0.1, 0.2],
    'min_child_weight': range(1,6,2)
}

"""- Goal metric - precision
- Why? - lower FP rate, less wrong decisions, can use model for  relabeling of the <organic> data users.
"""

grid_serach_xgboost = GridSearchCV(estimator = xgboost_model,
                                   param_grid = param_grid,
                                   verbose=10,
                                   scoring='precision',
                                   cv=3
                                   )

grid_serach_xgboost.fit(X_train, y_train.astype(int)-1)

print("Best parameters for precision:", grid_serach_xgboost.best_params_)
print("Best precision score:", grid_serach_xgboost.best_score_)

"""Best parameters for precision:

- {'learning_rate': 0.2, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 500, 'subsample': 0.7}
- best precision score: 0.7413683715935915
"""

xgboost_model = XGBClassifier(
    n_estimators=500,
    max_depth=7,
    subsample=0.7,
    grow_policy= 'depthwise',
    learning_rate=0.2,
    min_child_weight=5,
    random_state=42
)

xgboost_model.fit(X_train,y_train)

predictions = xgboost_model.predict_proba(X_test)

threshold = 0.5

y_pred = np.where(predictions[:,0]>threshold,0,1)

xgboostAccuracy = accuracy_score(y_true = y_test,
                                      y_pred = y_pred
                                      )

xgboostPrecision = precision_score(y_true = y_test,
                                        y_pred = y_pred
                                      )

xgboostRecall = recall_score(y_true = y_test,
                                  y_pred = y_pred
                                  )

xgboostRoc_auc_score = roc_auc_score(y_true = y_test,
                                          y_score = y_pred
                                  )

print(f"Accuracy : {xgboostAccuracy}, Precision : {xgboostPrecision}, Recall : {xgboostRecall}, ROC AUC Score {xgboostRoc_auc_score}")

cm = confusion_matrix(y_test, y_pred)
ConfusionMatrixDisplay(cm).plot()

fpr, tpr, thresholds = roc_curve(y_true=y_test,y_score=y_pred)
plt.figure()
plt.plot(fpr, tpr, label='ROC Curve')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

"""Feature engineering

Visualizing feature importances for XGBoost
"""

importances = xgboost_model.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(12,6))
plt.title('Feature importance in XGBoost model')
plt.bar(range(X_train.shape[1]), importances[indices], align='center')
plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.show()

importances[indices]

dataset.drop(columns=['avg_open_overs','avg_open_sport'],inplace=True)

"""Creating separate test dataset for feedback loop."""

X = dataset.drop(columns=['user_class'])
y = dataset['user_class']

X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.22, random_state=42)

xgboost_model = XGBClassifier(
    n_estimators=500,
    max_depth=7,
    subsample=0.7,
    grow_policy= 'depthwise',
    learning_rate=0.2,
    min_child_weight=5,
    random_state=42
)

xgboost_model.fit(X_train,y_train)

test_predictions = xgboost_model.predict_proba(X_val)

threshold = 0.5

y_pred = np.where(test_predictions[:,0]>threshold,0,1)

xgboostAccuracy = accuracy_score(y_true = y_val,
                                      y_pred = y_pred
                                      )

xgboostPrecision = precision_score(y_true = y_val,
                                        y_pred = y_pred
                                      )

xgboostRecall = recall_score(y_true = y_val,
                                  y_pred = y_pred
                                  )

xgboostRoc_auc_score = roc_auc_score(y_true = y_val,
                                          y_score = y_pred
                                  )
print("Validation set metrics : \n")
print(f"Accuracy : {xgboostAccuracy}, Precision : {xgboostPrecision}, Recall : {xgboostRecall}, ROC AUC Score {xgboostRoc_auc_score}")

cm = confusion_matrix(y_val, y_pred)
ConfusionMatrixDisplay(cm).plot()

fpr, tpr, thresholds = roc_curve(y_true=y_val,y_score=y_pred)
plt.figure()
plt.plot(fpr, tpr, label='ROC Curve')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

"""Defining safety metrics, where I will use these to get"""

safety_metrics = {'accuracy' : xgboostAccuracy,
                  'precision' : xgboostPrecision,
                  'recall' : xgboostRecall,
                  'roc_auc_score':xgboostRoc_auc_score
                  }
safety_metrics

test_predictions = xgboost_model.predict_proba(X_test)

threshold = 0.5

y_pred = np.where(test_predictions[:,0]>threshold,0,1)

xgboostAccuracy = accuracy_score(y_true = y_test,
                                      y_pred = y_pred
                                      )

xgboostPrecision = precision_score(y_true = y_test,
                                        y_pred = y_pred
                                      )

xgboostRecall = recall_score(y_true = y_test,
                                  y_pred = y_pred
                                  )

xgboostRoc_auc_score = roc_auc_score(y_true = y_test,
                                          y_score = y_pred
                                  )
print("Test set metrics : \n")
print(f"Accuracy : {xgboostAccuracy}, Precision : {xgboostPrecision}, Recall : {xgboostRecall}, ROC AUC Score {xgboostRoc_auc_score}")

cm = confusion_matrix(y_test, y_pred)
ConfusionMatrixDisplay(cm).plot()

fpr, tpr, thresholds = roc_curve(y_true=y_test,y_score=y_pred)
plt.figure()
plt.plot(fpr, tpr, label='ROC Curve')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

"""Defining test safety metrics, where I will use these to get"""

test_safety_metrics = {'accuracy' : xgboostAccuracy,
                  'precision' : xgboostPrecision,
                  'recall' : xgboostRecall,
                  'roc_auc_score':xgboostRoc_auc_score
                  }
test_safety_metrics

safety_metrics['f1_score'] = 2 * (safety_metrics['precision'] * safety_metrics['recall']) / (safety_metrics['precision'] + safety_metrics['recall'])

test_safety_metrics['f1_score'] = 2 * (test_safety_metrics['precision'] * test_safety_metrics['recall']) / (test_safety_metrics['precision'] + test_safety_metrics['recall'])

"""Evaluation on train dataset"""

train_predictions = xgboost_model.predict_proba(X_train)

threshold = 0.5

y_pred = np.where(train_predictions[:,0]>threshold,0,1)

xgboostAccuracy = accuracy_score(y_true = y_train,
                                      y_pred = y_pred
                                      )

xgboostPrecision = precision_score(y_true = y_train,
                                        y_pred = y_pred
                                      )

xgboostRecall = recall_score(y_true = y_train,
                                  y_pred = y_pred
                                  )

xgboostRoc_auc_score = roc_auc_score(y_true = y_train,
                                          y_score = y_pred
                                  )

print("Train set metrics : \n")
print(f"Accuracy : {xgboostAccuracy}, Precision : {xgboostPrecision}, Recall : {xgboostRecall}, ROC AUC Score {xgboostRoc_auc_score}")

cm = confusion_matrix(y_train, y_pred)
ConfusionMatrixDisplay(cm).plot()

fpr, tpr, thresholds = roc_curve(y_true=y_train,y_score=y_pred)
plt.figure()
plt.plot(fpr, tpr, label='ROC Curve')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

"""Using higher threshold, what I am saying is this: if the model is uncertain about the class, rather predict the class 2, for which we have correct labels.
As can be seen, performance on train and valid dataset is similar.
Now. I will create feedback loop; I will use model certaintly and over n iterations use this logic: for all the samples for which the model is uncertain for, try changing labels. Evaluate such mod
el and if target metric is better, keep labels and repeat procedure n times.
"""

from sklearn.metrics import f1_score

X_orig = X_train
y_orig = y_train

# X_train = X_orig
# y_train = y_orig

uncertain_indexes = np.where((train_predictions[:, 0] > 0.5) & (train_predictions[:, 0] < 0.55))[0]

print(f"Length of the changed labels : {len(uncertain_indexes)}")

y_train.iloc[uncertain_indexes] = 1

lower_threshold, upper_threshold = 0.5,0.6

val_predictions = xgboost_model.predict_proba(X_val)

best_val_f1_score = safety_metrics['f1_score']
best_test_f1_score = test_safety_metrics['f1_score']

lower_threshold, upper_threshold = 0.5, 0.6
iterations = 10

for iteration in range(iterations):
    train_predictions = xgboost_model.predict_proba(X_train)
    xgboost_model.fit(X_train, y_train)

    val_predictions = xgboost_model.predict_proba(X_val)
    y_pred_val = np.where(val_predictions[:, 0] > 0.5, 0, 1)
    current_val_f1_score = f1_score(y_val, y_pred_val)

    if current_val_f1_score > best_val_f1_score:

        test_predictions = xgboost_model.predict_proba(X_test)
        y_pred_test = np.where(test_predictions[:, 0] > 0.5, 0, 1)
        current_test_f1_score = f1_score(y_test, y_pred_test)

        if current_test_f1_score > best_test_f1_score:
            print("Both validation and test F1 scores improved.")
            best_val_f1_score = current_val_f1_score
            best_test_f1_score = current_test_f1_score
            uncertain_indexes = np.where((train_predictions[:, 0] > lower_threshold) & (train_predictions[:, 0] < upper_threshold))[0]
            print(f"Length of the changed labels : {len(uncertain_indexes)}")
            y_train.iloc[uncertain_indexes] = 1
        else:
            print("Validation improved but not test.")
            print("Stopping.")
            break
    else:
        print("No improvement in validation F1 score.")
        print("Stopping.")
        break

    print(f"Iteration {iteration}: vlidation F1 Score: {current_val_f1_score}, test F1 Score: {current_test_f1_score}\n")

"""In 3 iterations, validation and test set had seen improvement, and changed labels should be considered.

I will plot results on the train, val and test set.
"""

print(f"Newly recorded best f1 on independant test dataset: {best_test_f1_score}, previously best: {test_safety_metrics['f1_score']}")
print(f"Newly recorded best f1 on val dataset: {best_val_f1_score}, previously best: {safety_metrics['f1_score']}")

xgboost_model.fit(X_train,y_train)

test_predictions = xgboost_model.predict_proba(X_test)

threshold = 0.5

y_pred = np.where(test_predictions[:,0]>threshold,0,1)

xgboostAccuracy = accuracy_score(y_true = y_test,
                                      y_pred = y_pred
                                      )

xgboostPrecision = precision_score(y_true = y_test,
                                        y_pred = y_pred
                                      )

xgboostRecall = recall_score(y_true = y_test,
                                  y_pred = y_pred
                                  )

xgboostRoc_auc_score = roc_auc_score(y_true = y_test,
                                          y_score = y_pred
                                  )
print("Test set metrics : \n")
print(f"Accuracy : {xgboostAccuracy}, Precision : {xgboostPrecision}, Recall : {xgboostRecall}, ROC AUC Score {xgboostRoc_auc_score}")

cm = confusion_matrix(y_test, y_pred)
ConfusionMatrixDisplay(cm).plot()

fpr, tpr, thresholds = roc_curve(y_true=y_test,y_score=y_predictions)
plt.figure()
plt.plot(fpr, tpr, label='ROC Curve')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

"""Alternative : Using KFold cross validation"""

from sklearn.model_selection import StratifiedKFold, train_test_split

n_splits = 5
n_iterations = 10
threshold_range = np.linspace(0.25, 0.85, 11)

kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

model_metrics = []

best_model = None
overall_best_f1 = -1

for train_index, test_index in kf.split(X, y):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    y_train = y_train.astype(int)
    y_test = y_test.astype(int)

    model = XGBClassifier(n_estimators=500, max_depth=7, subsample=0.7, grow_policy='depthwise',
                          learning_rate=0.2, min_child_weight=5, random_state=42)
    model.fit(X_train, y_train)

    test_predictions_proba = model.predict_proba(X_test)[:, 1]
    best_f1 = 0
    best_threshold = 0.5
    for threshold in threshold_range:
        y_pred = (test_predictions_proba > threshold).astype(int)
        f1 = f1_score(y_test, y_pred)
        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold

    if best_f1 > overall_best_f1:
      overall_best_f1 = best_f1
      best_model = model

    y_pred = (test_predictions_proba > best_threshold).astype(int)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred)

    model_metrics.append((accuracy, precision, recall, roc_auc, best_threshold, best_f1))


print("Model performance metrics:")
for metrics in model_metrics:
    print(f"Accuracy: {metrics[0]}, Precision: {metrics[1]}, Recall: {metrics[2]}, ROC AUC: {metrics[3]}, Optimal Threshold: {metrics[4]}, F1 Score: {metrics[5]}")

fpr, tpr, _ = roc_curve(y_test, test_predictions_proba)
plt.figure()
plt.plot(fpr, tpr, label='ROC Curve')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

overall_best_f1

import joblib
if best_model is not None:
    joblib.dump(best_model, 'best_model.joblib')

"""CONCLUSIONS:

- overall, best model proved to be XGBoost
- only 2 features were removed; 'avg_open_overs','avg_open_sport'
- models were good at predicting campaign class
- no feature were scaled (even though scaling would have helped in logistic regression for instance) due to nature of XGBoost, where the data doesn't have to be scaled
- semi-supervied model training did show some progress, but the best model was obtained using stratified k fold sampling, after which the model achieved the highest F1 score, and that model is serialized. Moreover, in that code I added iterating over linear space of threshold, to check which threshold is the best to set for predicting classes, given the uncertainty of the data.
- final model logs : F1 Score 82%, Accuracy: 7/%, Precision: 72%, Recall: 95%
"""